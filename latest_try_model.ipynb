{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "331138a8-6032-4371-8fe3-0f0b9f6489f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# train_severity_pipeline.py\n",
    "\n",
    "# End-to-end pipeline:\n",
    "# - Load CSV (expects columns: 'processed_text' and 'severity_score')\n",
    "# - Preprocess (lowercase, remove URLs/HTML, remove punctuation/numbers, KEEP stopwords)\n",
    "# - Extract Sentence-BERT embeddings (all-MiniLM-L6-v2)\n",
    "# - Extract psycholinguistic features using Empath (open alternative to LIWC)\n",
    "# - Concatenate features, train LightGBM regressor\n",
    "# - Evaluate and save models/artifacts\n",
    "# - Provide infer_severity(text) function for single-text inference\n",
    "\n",
    "# Requirements (install before running):\n",
    "# pip install pandas numpy tqdm sentence-transformers empath lightgbm scikit-learn joblib nltk\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import re\n",
    "# import json\n",
    "# import argparse\n",
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import joblib\n",
    "# import logging\n",
    "\n",
    "# # NLP libs\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# nltk.download('punkt', quiet=True)\n",
    "\n",
    "# # Sentence Embeddings\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # Empath (psycholinguistic features)\n",
    "# from empath import Empath\n",
    "\n",
    "# # ML\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import lightgbm as lgb\n",
    "\n",
    "# # Setup logging\n",
    "# logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # ---------------------------\n",
    "# # Config / Paths (edit below)\n",
    "# # ---------------------------\n",
    "# INPUT_CSV = \"reddit_with_post_severity.csv\"    # expects columns: processed_text, severity_score\n",
    "# OUTPUT_DIR = \"models_pipeline\"\n",
    "# EMBED_CACHE = os.path.join(OUTPUT_DIR, \"embeddings.npy\")\n",
    "# EMPATH_CACHE = os.path.join(OUTPUT_DIR, \"empath_feats.npy\")\n",
    "# SYMBOLS_CACHE = os.path.join(OUTPUT_DIR, \"empath_cols.json\")\n",
    "# MODEL_PATH = os.path.join(OUTPUT_DIR, \"lightgbm_severity.pkl\")\n",
    "# SCALER_PATH = os.path.join(OUTPUT_DIR, \"feature_scaler.pkl\")\n",
    "# EMBEDDER_PATH = os.path.join(OUTPUT_DIR, \"embedder_all-MiniLM.pkl\")  # optionally save embedder\n",
    "# EMPATHOR_PATH = os.path.join(OUTPUT_DIR, \"empath_lexicon.pkl\")\n",
    "# SEED = 42\n",
    "\n",
    "# # ---------------------------\n",
    "# # Preprocessing\n",
    "# # ---------------------------\n",
    "# def preprocess_keep_stopwords(text):\n",
    "#     \"\"\"\n",
    "#     Lowercase, remove URLs and HTML, remove punctuation/numbers,\n",
    "#     tokenize (keeps stopwords).\n",
    "#     Returns cleaned string (tokens joined by spaces).\n",
    "#     \"\"\"\n",
    "#     if not isinstance(text, str):\n",
    "#         text = str(text) if text is not None else \"\"\n",
    "#     text = text.lower()\n",
    "#     # remove URLs\n",
    "#     text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "#     # remove HTML tags\n",
    "#     text = re.sub(r'<[^>]+>', ' ', text)\n",
    "#     # remove characters that are not letters or spaces (keep spaces)\n",
    "#     # we intentionally remove numbers/punctuation\n",
    "#     text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "#     # collapse whitespace\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()\n",
    "#     # tokenize to preserve consistent tokenization (but we return joined string)\n",
    "#     tokens = word_tokenize(text)\n",
    "#     return \" \".join(tokens)\n",
    "\n",
    "# # ---------------------------\n",
    "# # Empath features\n",
    "# # ---------------------------\n",
    "# def build_empath_features(texts, empath_analyzer=None, cache_path=None):\n",
    "#     \"\"\"\n",
    "#     texts: list of strings\n",
    "#     empath_analyzer: Empath() instance (if None, it will be created)\n",
    "#     Returns:\n",
    "#         - features_array: np.ndarray shape (n_texts, n_features)\n",
    "#         - columns: list of feature names\n",
    "#     Caches results to disk if cache_path provided.\n",
    "#     \"\"\"\n",
    "#     if empath_analyzer is None:\n",
    "#         empath_analyzer = Empath()\n",
    "#     if cache_path and os.path.exists(cache_path):\n",
    "#         logger.info(\"Loading empath features from cache: %s\", cache_path)\n",
    "#         feats = np.load(cache_path, allow_pickle=True)\n",
    "#         columns = json.load(open(SYMBOLS_CACHE, 'r'))\n",
    "#         return feats, columns\n",
    "\n",
    "#     logger.info(\"Computing Empath features for %d texts...\", len(texts))\n",
    "#     rows = []\n",
    "#     # Empath returns a dict of categories â†’ counts or normalized values (we ask normalize=True)\n",
    "#     for t in tqdm(texts, desc=\"Empath features\"):\n",
    "#         try:\n",
    "#             d = empath_analyzer.analyze(t, normalize=True)\n",
    "#         except Exception:\n",
    "#             # fallback to zeros for safety\n",
    "#             d = {}\n",
    "#         rows.append(d)\n",
    "#     df = pd.DataFrame(rows).fillna(0.0)\n",
    "#     columns = list(df.columns)\n",
    "#     feats = df.values.astype(float)\n",
    "\n",
    "#     if cache_path:\n",
    "#         os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "#         np.save(cache_path, feats)\n",
    "#         json.dump(columns, open(SYMBOLS_CACHE, 'w'))\n",
    "#         logger.info(\"Saved Empath cache -> %s and columns -> %s\", cache_path, SYMBOLS_CACHE)\n",
    "\n",
    "#     return feats, columns\n",
    "\n",
    "# # ---------------------------\n",
    "# # Embeddings extraction\n",
    "# # ---------------------------\n",
    "# def compute_or_load_embeddings(texts, model_name=\"all-MiniLM-L6-v2\", cache_path=None, device=None):\n",
    "#     \"\"\"\n",
    "#     Returns numpy array of embeddings (n_texts, dim).\n",
    "#     If cache exists, loads it.\n",
    "#     \"\"\"\n",
    "#     if cache_path and os.path.exists(cache_path):\n",
    "#         logger.info(\"Loading embeddings from cache: %s\", cache_path)\n",
    "#         return np.load(cache_path)\n",
    "\n",
    "#     logger.info(\"Loading SentenceTransformer model: %s\", model_name)\n",
    "#     embedder = SentenceTransformer(model_name, device=device)\n",
    "#     # batch encode\n",
    "#     logger.info(\"Encoding %d texts with embedder...\", len(texts))\n",
    "#     embeddings = embedder.encode(texts, batch_size=64, show_progress_bar=True)\n",
    "#     embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "#     if cache_path:\n",
    "#         os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "#         np.save(cache_path, embeddings)\n",
    "#         try:\n",
    "#             joblib.dump(embedder, EMBEDDER_PATH)\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#         logger.info(\"Saved embeddings cache -> %s\", cache_path)\n",
    "\n",
    "#     return embeddings\n",
    "\n",
    "# # ---------------------------\n",
    "# # Combine and train\n",
    "# # ---------------------------\n",
    "# def train_model(X, y, output_model_path=MODEL_PATH, output_scaler_path=SCALER_PATH, test_size=0.2):\n",
    "#     \"\"\"\n",
    "#     Trains a LightGBM regressor on X,y. Saves model and scaler.\n",
    "#     Returns trained model and scaler and (X_test,y_test,y_pred).\n",
    "#     \"\"\"\n",
    "#     logger.info(\"Splitting data (test_size=%s)...\", test_size)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=SEED)\n",
    "\n",
    "#     logger.info(\"Scaling features (StandardScaler)...\")\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#     logger.info(\"Training LightGBM regressor...\")\n",
    "#     params = {\n",
    "#         'objective': 'regression',\n",
    "#         'metric': 'rmse',\n",
    "#         'verbosity': -1,\n",
    "#         'random_state': SEED\n",
    "#     }\n",
    "\n",
    "#     # Use sklearn API for simpler save/load\n",
    "#     model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=31, random_state=SEED)\n",
    "#     model.fit(\n",
    "#         X_train_scaled, y_train,\n",
    "#         eval_set=[(X_test_scaled, y_test)],\n",
    "#         eval_metric='rmse',\n",
    "#         early_stopping_rounds=50,\n",
    "#         verbose=50\n",
    "#     )\n",
    "\n",
    "#     logger.info(\"Predicting on test set...\")\n",
    "#     y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "#     logger.info(\"Saving model and scaler...\")\n",
    "#     os.makedirs(os.path.dirname(output_model_path), exist_ok=True)\n",
    "#     joblib.dump(model, output_model_path)\n",
    "#     joblib.dump(scaler, output_scaler_path)\n",
    "#     logger.info(\"Model saved to %s\", output_model_path)\n",
    "#     logger.info(\"Scaler saved to %s\", output_scaler_path)\n",
    "\n",
    "#     return model, scaler, (X_test, y_test, y_pred)\n",
    "\n",
    "# # ---------------------------\n",
    "# # Evaluation utilities\n",
    "# # ---------------------------\n",
    "# def evaluate_preds(y_true, y_pred):\n",
    "#     mse = mean_squared_error(y_true, y_pred)\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     mae = mean_absolute_error(y_true, y_pred)\n",
    "#     r2 = r2_score(y_true, y_pred)\n",
    "#     logger.info(\"Evaluation -> RMSE: %.4f | MAE: %.4f | R2: %.4f\", rmse, mae, r2)\n",
    "#     return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "# # ---------------------------\n",
    "# # Inference\n",
    "# # ---------------------------\n",
    "# def load_artifacts(model_path=MODEL_PATH, scaler_path=SCALER_PATH, embed_cache=EMBED_CACHE, empath_cols_path=SYMBOLS_CACHE):\n",
    "#     \"\"\"Load trained model and scaler. Also load embedder from disk if available.\"\"\"\n",
    "#     model = joblib.load(model_path)\n",
    "#     scaler = joblib.load(scaler_path)\n",
    "#     # load columns for empath space\n",
    "#     if os.path.exists(empath_cols_path):\n",
    "#         empath_cols = json.load(open(empath_cols_path, 'r'))\n",
    "#     else:\n",
    "#         empath_cols = None\n",
    "#     embedder = None\n",
    "#     try:\n",
    "#         if os.path.exists(EMBEDDER_PATH):\n",
    "#             embedder = joblib.load(EMBEDDER_PATH)\n",
    "#     except Exception:\n",
    "#         embedder = None\n",
    "#     return model, scaler, embedder, empath_cols\n",
    "\n",
    "# def infer_severity(text, model, scaler, embedder, empath_cols, empath_analyzer=None):\n",
    "#     \"\"\"Single-text inference. Returns severity score (0-1).\"\"\"\n",
    "#     # preprocess\n",
    "#     txt = preprocess_keep_stopwords(text)\n",
    "#     # embed\n",
    "#     if embedder is None:\n",
    "#         embedder = SentenceTransformer('all-MiniLM-L6-v2')  # fallback\n",
    "#     emb = embedder.encode([txt])\n",
    "#     emb = np.array(emb, dtype=np.float32)\n",
    "#     # empath features\n",
    "#     if empath_analyzer is None:\n",
    "#         empath_analyzer = Empath()\n",
    "#     empath_dict = empath_analyzer.analyze(txt, normalize=True)\n",
    "#     # ensure columns order\n",
    "#     if empath_cols is None:\n",
    "#         empath_cols = sorted(list(empath_dict.keys()))\n",
    "#     empath_row = [empath_dict.get(c, 0.0) for c in empath_cols]\n",
    "#     empath_row = np.array(empath_row, dtype=np.float32).reshape(1, -1)\n",
    "#     # combine\n",
    "#     X_new = np.hstack([emb, empath_row])\n",
    "#     # scale\n",
    "#     X_new_scaled = scaler.transform(X_new)\n",
    "#     # predict\n",
    "#     pred = model.predict(X_new_scaled)[0]\n",
    "#     return float(max(0.0, min(1.0, pred)))\n",
    "\n",
    "# # ---------------------------\n",
    "# # Main execution\n",
    "# # ---------------------------\n",
    "# def main(input_csv=INPUT_CSV, overwrite_cache=False, test_size=0.2, device=None):\n",
    "#     logger.info(\"Loading input CSV: %s\", input_csv)\n",
    "#     df = pd.read_csv(input_csv)\n",
    "\n",
    "#     # Expect columns\n",
    "#     if 'processed_text' not in df.columns or 'severity_score' not in df.columns:\n",
    "#         raise ValueError(\"Input CSV must contain 'processed_text' and 'severity_score' columns\")\n",
    "\n",
    "#     # Preprocess (ensure stopwords are kept)\n",
    "#     logger.info(\"Preprocessing texts (keep stopwords)...\")\n",
    "#     df['processed_text_clean'] = df['processed_text'].fillna(\"\").astype(str).apply(preprocess_keep_stopwords)\n",
    "\n",
    "#     texts = df['processed_text_clean'].tolist()\n",
    "#     y = df['severity_score'].astype(float).values\n",
    "\n",
    "#     # embeddings\n",
    "#     if not overwrite_cache and os.path.exists(EMBED_CACHE):\n",
    "#         embeddings = np.load(EMBED_CACHE)\n",
    "#     else:\n",
    "#         embeddings = compute_or_load_embeddings(texts, cache_path=EMBED_CACHE, device=device)\n",
    "\n",
    "#     # empath features\n",
    "#     if not overwrite_cache and os.path.exists(EMPATH_CACHE) and os.path.exists(SYMBOLS_CACHE):\n",
    "#         empath_feats = np.load(EMPATH_CACHE, allow_pickle=True)\n",
    "#         empath_cols = json.load(open(SYMBOLS_CACHE, 'r'))\n",
    "#     else:\n",
    "#         empath_analyzer = Empath()\n",
    "#         empath_feats, empath_cols = build_empath_features(texts, empath_analyzer=empath_analyzer, cache_path=EMPATH_CACHE)\n",
    "\n",
    "#     # Combine features (align dims)\n",
    "#     logger.info(\"Combining embeddings (%s) + empath features (%s)...\", embeddings.shape, empath_feats.shape)\n",
    "#     X = np.hstack([embeddings, empath_feats])\n",
    "\n",
    "#     # Train model\n",
    "#     model, scaler, test_triplet = train_model(X, y, output_model_path=MODEL_PATH, output_scaler_path=SCALER_PATH, test_size=test_size)\n",
    "#     X_test, y_test, y_pred = test_triplet\n",
    "\n",
    "#     # Evaluate\n",
    "#     eval_stats = evaluate_preds(y_test, y_pred)\n",
    "\n",
    "#     # Save additional artifacts (empath columns)\n",
    "#     os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "#     json.dump({'empath_cols': empath_cols, 'embed_dim': embeddings.shape[1]}, open(os.path.join(OUTPUT_DIR, \"meta.json\"), 'w'))\n",
    "\n",
    "#     logger.info(\"Pipeline finished. Model, scaler and artifacts saved to %s\", OUTPUT_DIR)\n",
    "#     return model, scaler, embeddings, empath_feats, empath_cols, eval_stats\n",
    "\n",
    "# # ---------------------------\n",
    "# # If run as script\n",
    "# # ---------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description=\"Train severity model: embeddings + empath + LightGBM\")\n",
    "#     parser.add_argument(\"--input\", type=str, default=INPUT_CSV, help=\"Input CSV path (must contain processed_text and severity_score)\")\n",
    "#     parser.add_argument(\"--outdir\", type=str, default=OUTPUT_DIR, help=\"Output folder for models and caches\")\n",
    "#     parser.add_argument(\"--overwrite\", action=\"store_true\", help=\"Overwrite caches (recompute embeddings/empath)\")\n",
    "#     parser.add_argument(\"--test_size\", type=float, default=0.2, help=\"Test split fraction\")\n",
    "#     parser.add_argument(\"--device\", type=str, default=None, help=\"Device for sentence-transformers (e.g. 'cuda' or 'cpu')\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     # update global paths based on outdir\n",
    "#     OUTPUT_DIR = args.outdir\n",
    "#     EMBED_CACHE = os.path.join(OUTPUT_DIR, \"embeddings.npy\")\n",
    "#     EMPATH_CACHE = os.path.join(OUTPUT_DIR, \"empath_feats.npy\")\n",
    "#     SYMBOLS_CACHE = os.path.join(OUTPUT_DIR, \"empath_cols.json\")\n",
    "#     MODEL_PATH = os.path.join(OUTPUT_DIR, \"lightgbm_severity.pkl\")\n",
    "#     SCALER_PATH = os.path.join(OUTPUT_DIR, \"feature_scaler.pkl\")\n",
    "#     EMBEDDER_PATH = os.path.join(OUTPUT_DIR, \"embedder_all-MiniLM.pkl\")\n",
    "#     os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "#     main(input_csv=args.input, overwrite_cache=args.overwrite, test_size=args.test_size, device=args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd0bd187-4330-46b3-acab-facfa23db350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m df \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/reddit_with_post_severity.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Only run BERT & emotion mapping if not already in CSV\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m df \u001b[38;5;241m=\u001b[39m get_bert_features(df)\n\u001b[0;32m     93\u001b[0m df \u001b[38;5;241m=\u001b[39m get_psycholinguistic_features(df)\n\u001b[0;32m     95\u001b[0m model, tfidf \u001b[38;5;241m=\u001b[39m train_model(df)\n",
      "Cell \u001b[1;32mIn[14], line 35\u001b[0m, in \u001b[0;36mget_bert_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     32\u001b[0m sentiments\u001b[38;5;241m.\u001b[39mappend(sentiment_score)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Emotion\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m emo_res \u001b[38;5;241m=\u001b[39m emotion_classifier(text[:\u001b[38;5;241m512\u001b[39m])\n\u001b[0;32m     36\u001b[0m emo_dict \u001b[38;5;241m=\u001b[39m {e[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]: e[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m emo_res[\u001b[38;5;241m0\u001b[39m]}\n\u001b[0;32m     37\u001b[0m emotions\u001b[38;5;241m.\u001b[39mappend(emo_dict)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:163\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[1;32m--> 163\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1464\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1457\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1458\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1461\u001b[0m         )\n\u001b[0;32m   1462\u001b[0m     )\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1471\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1470\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1471\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1472\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1371\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1370\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1371\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1372\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:194\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    193\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1191\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;124;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;124;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1191\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta(\n\u001b[0;32m   1192\u001b[0m     input_ids,\n\u001b[0;32m   1193\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1194\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1195\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1196\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1197\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1198\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1199\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1200\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1201\u001b[0m )\n\u001b[0;32m   1202\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1203\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:858\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    856\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 858\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    859\u001b[0m     embedding_output,\n\u001b[0;32m    860\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m    861\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    862\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    863\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m    864\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    865\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    866\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    867\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    868\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    869\u001b[0m )\n\u001b[0;32m    870\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    871\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:607\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    604\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    605\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 607\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    608\u001b[0m     hidden_states,\n\u001b[0;32m    609\u001b[0m     attention_mask,\n\u001b[0;32m    610\u001b[0m     layer_head_mask,\n\u001b[0;32m    611\u001b[0m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[0;32m    612\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    613\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    614\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    615\u001b[0m )\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:508\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    498\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    505\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    507\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 508\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    509\u001b[0m         hidden_states,\n\u001b[0;32m    510\u001b[0m         attention_mask,\n\u001b[0;32m    511\u001b[0m         head_mask,\n\u001b[0;32m    512\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    513\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    514\u001b[0m     )\n\u001b[0;32m    515\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:435\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    427\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    433\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    434\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 435\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    436\u001b[0m         hidden_states,\n\u001b[0;32m    437\u001b[0m         attention_mask,\n\u001b[0;32m    438\u001b[0m         head_mask,\n\u001b[0;32m    439\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    440\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    441\u001b[0m         past_key_value,\n\u001b[0;32m    442\u001b[0m         output_attentions,\n\u001b[0;32m    443\u001b[0m     )\n\u001b[0;32m    444\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    445\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:327\u001b[0m, in \u001b[0;36mRobertaSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(current_states))\n\u001b[1;32m--> 327\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(current_states))\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention:\n\u001b[0;32m    329\u001b[0m         key_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m0\u001b[39m], key_layer], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# =====================\n",
    "# 1. Load your dataset\n",
    "# =====================\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'processed_text' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain a 'processed_text' column\")\n",
    "    return df\n",
    "\n",
    "# =====================\n",
    "# 2. BERT/RoBERTa Sentiment & Emotion\n",
    "# =====================\n",
    "def get_bert_features(df):\n",
    "    sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "    emotion_classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", top_k=None)\n",
    "\n",
    "    sentiments = []\n",
    "    emotions = []\n",
    "    \n",
    "    for text in df['processed_text']:\n",
    "        # Sentiment\n",
    "        sent_res = sentiment_analyzer(text[:512])[0]  # limit to 512 tokens\n",
    "        sentiment_score = sent_res['score'] if sent_res['label'] == 'POSITIVE' else -sent_res['score']\n",
    "        sentiments.append(sentiment_score)\n",
    "\n",
    "        # Emotion\n",
    "        emo_res = emotion_classifier(text[:512])\n",
    "        emo_dict = {e['label']: e['score'] for e in emo_res[0]}\n",
    "        emotions.append(emo_dict)\n",
    "\n",
    "    df['bert_sentiment'] = sentiments\n",
    "    emotion_df = pd.DataFrame(emotions).fillna(0)\n",
    "    df = pd.concat([df, emotion_df], axis=1)\n",
    "    return df\n",
    "\n",
    "# =====================\n",
    "# 3. LIWC & DLATK Features (placeholder)\n",
    "# =====================\n",
    "def get_psycholinguistic_features(df):\n",
    "    # Placeholder: Replace with LIWC/DLATK output merge\n",
    "    df['liwc_self_ref'] = np.random.rand(len(df))\n",
    "    df['liwc_affect'] = np.random.rand(len(df))\n",
    "    df['dlatk_cognition'] = np.random.rand(len(df))\n",
    "    return df\n",
    "\n",
    "# =====================\n",
    "# 4. Train Ridge Regression Model\n",
    "# =====================\n",
    "def train_model(df):\n",
    "    X_text = df['processed_text']\n",
    "    y = df['severity_score']\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=5000)\n",
    "    X_tfidf = tfidf.fit_transform(X_text)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_tfidf, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Test MSE: {mean_squared_error(y_test, y_pred):.4f}\")\n",
    "    print(f\"Test RÂ²: {r2_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "    return model, tfidf\n",
    "\n",
    "# =====================\n",
    "# 5. Predict severity for new text\n",
    "# =====================\n",
    "def predict_severity(text, model, tfidf):\n",
    "    text_vec = tfidf.transform([text])\n",
    "    score = model.predict(text_vec)[0]\n",
    "    return max(0.0, min(1.0, score))\n",
    "\n",
    "# =====================\n",
    "# Run everything in Jupyter\n",
    "# =====================\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data(\"output/reddit_with_post_severity.csv\")\n",
    "    \n",
    "    # Only run BERT & emotion mapping if not already in CSV\n",
    "    df = get_bert_features(df)\n",
    "    df = get_psycholinguistic_features(df)\n",
    "\n",
    "    model, tfidf = train_model(df)\n",
    "\n",
    "    # Example prediction\n",
    "    sample_text = \"I feel so hopeless and alone.\"\n",
    "    print(f\"Predicted severity: {predict_severity(sample_text, model, tfidf):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da6f59c-41eb-443e-88bf-d30ae5fee2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_severity_pipeline.py\n",
    "\n",
    "End-to-end pipeline:\n",
    "- Load CSV (expects columns: 'processed_text' and 'severity_score')\n",
    "- Preprocess (lowercase, remove URLs/HTML, remove punctuation/numbers, KEEP stopwords)\n",
    "- Extract Sentence-BERT embeddings (all-MiniLM-L6-v2)\n",
    "- Extract psycholinguistic features using Empath (open alternative to LIWC)\n",
    "- Concatenate features, train LightGBM regressor\n",
    "- Evaluate and save models/artifacts\n",
    "- Provide infer_severity(text) function for single-text inference\n",
    "\n",
    "Requirements (install before running):\n",
    "pip install pandas numpy tqdm sentence-transformers empath lightgbm scikit-learn joblib nltk\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "# NLP libs\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Sentence Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Empath (psycholinguistic features)\n",
    "from empath import Empath\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------\n",
    "# Config / Paths (edit below)\n",
    "# ---------------------------\n",
    "INPUT_CSV = \"reddit_with_post_severity.csv\"    # expects columns: processed_text, severity_score\n",
    "OUTPUT_DIR = \"models_pipeline\"\n",
    "EMBED_CACHE = os.path.join(OUTPUT_DIR, \"embeddings.npy\")\n",
    "EMPATH_CACHE = os.path.join(OUTPUT_DIR, \"empath_feats.npy\")\n",
    "SYMBOLS_CACHE = os.path.join(OUTPUT_DIR, \"empath_cols.json\")\n",
    "MODEL_PATH = os.path.join(OUTPUT_DIR, \"lightgbm_severity.pkl\")\n",
    "SCALER_PATH = os.path.join(OUTPUT_DIR, \"feature_scaler.pkl\")\n",
    "EMBEDDER_PATH = os.path.join(OUTPUT_DIR, \"embedder_all-MiniLM.pkl\")  # optionally save embedder\n",
    "EMPATHOR_PATH = os.path.join(OUTPUT_DIR, \"empath_lexicon.pkl\")\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7167fe46-0f98-4178-993f-484301268ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Preprocessing\n",
    "# ---------------------------\n",
    "def preprocess_keep_stopwords(text):\n",
    "    \"\"\"\n",
    "    Lowercase, remove URLs and HTML, remove punctuation/numbers,\n",
    "    tokenize (keeps stopwords).\n",
    "    Returns cleaned string (tokens joined by spaces).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text) if text is not None else \"\"\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    # remove characters that are not letters or spaces (keep spaces)\n",
    "    # we intentionally remove numbers/punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    # collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # tokenize to preserve consistent tokenization (but we return joined string)\n",
    "    tokens = word_tokenize(text)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62fea115-f86b-4d01-9dd1-1ff19a052598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Empath features\n",
    "# ---------------------------\n",
    "def build_empath_features(texts, empath_analyzer=None, cache_path=None):\n",
    "    \"\"\"\n",
    "    texts: list of strings\n",
    "    empath_analyzer: Empath() instance (if None, it will be created)\n",
    "    Returns:\n",
    "        - features_array: np.ndarray shape (n_texts, n_features)\n",
    "        - columns: list of feature names\n",
    "    Caches results to disk if cache_path provided.\n",
    "    \"\"\"\n",
    "    if empath_analyzer is None:\n",
    "        empath_analyzer = Empath()\n",
    "    if cache_path and os.path.exists(cache_path):\n",
    "        logger.info(\"Loading empath features from cache: %s\", cache_path)\n",
    "        feats = np.load(cache_path, allow_pickle=True)\n",
    "        columns = json.load(open(SYMBOLS_CACHE, 'r'))\n",
    "        return feats, columns\n",
    "\n",
    "    logger.info(\"Computing Empath features for %d texts...\", len(texts))\n",
    "    rows = []\n",
    "    # Empath returns a dict of categories â†’ counts or normalized values (we ask normalize=True)\n",
    "    for t in tqdm(texts, desc=\"Empath features\"):\n",
    "        try:\n",
    "            d = empath_analyzer.analyze(t, normalize=True)\n",
    "        except Exception:\n",
    "            # fallback to zeros for safety\n",
    "            d = {}\n",
    "        rows.append(d)\n",
    "    df = pd.DataFrame(rows).fillna(0.0)\n",
    "    columns = list(df.columns)\n",
    "    feats = df.values.astype(float)\n",
    "\n",
    "    if cache_path:\n",
    "        os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "        np.save(cache_path, feats)\n",
    "        json.dump(columns, open(SYMBOLS_CACHE, 'w'))\n",
    "        logger.info(\"Saved Empath cache -> %s and columns -> %s\", cache_path, SYMBOLS_CACHE)\n",
    "\n",
    "    return feats, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ea9f10-a988-46e0-abc5-e76207904e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Embeddings extraction\n",
    "# ---------------------------\n",
    "def compute_or_load_embeddings(texts, model_name=\"all-MiniLM-L6-v2\", cache_path=None, device=None):\n",
    "    \"\"\"\n",
    "    Returns numpy array of embeddings (n_texts, dim).\n",
    "    If cache exists, loads it.\n",
    "    \"\"\"\n",
    "    if cache_path and os.path.exists(cache_path):\n",
    "        logger.info(\"Loading embeddings from cache: %s\", cache_path)\n",
    "        return np.load(cache_path)\n",
    "\n",
    "    logger.info(\"Loading SentenceTransformer model: %s\", model_name)\n",
    "    embedder = SentenceTransformer(model_name, device=device)\n",
    "    # batch encode\n",
    "    logger.info(\"Encoding %d texts with embedder...\", len(texts))\n",
    "    embeddings = embedder.encode(texts, batch_size=64, show_progress_bar=True)\n",
    "    embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "    if cache_path:\n",
    "        os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "        np.save(cache_path, embeddings)\n",
    "        try:\n",
    "            joblib.dump(embedder, EMBEDDER_PATH)\n",
    "        except Exception:\n",
    "            pass\n",
    "        logger.info(\"Saved embeddings cache -> %s\", cache_path)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7441e5f-6ee9-4ddc-a4a7-c786a90a2c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Combine and train\n",
    "# ---------------------------\n",
    "def train_model(X, y, output_model_path=MODEL_PATH, output_scaler_path=SCALER_PATH, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Trains a LightGBM regressor on X,y. Saves model and scaler.\n",
    "    Returns trained model and scaler and (X_test,y_test,y_pred).\n",
    "    \"\"\"\n",
    "    logger.info(\"Splitting data (test_size=%s)...\", test_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=SEED)\n",
    "\n",
    "    logger.info(\"Scaling features (StandardScaler)...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    logger.info(\"Training LightGBM regressor...\")\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'random_state': SEED\n",
    "    }\n",
    "\n",
    "    # Use sklearn API for simpler save/load\n",
    "    model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=31, random_state=SEED)\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        eval_set=[(X_test_scaled, y_test)],\n",
    "        eval_metric='rmse',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=50\n",
    "    )\n",
    "\n",
    "    logger.info(\"Predicting on test set...\")\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    logger.info(\"Saving model and scaler...\")\n",
    "    os.makedirs(os.path.dirname(output_model_path), exist_ok=True)\n",
    "    joblib.dump(model, output_model_path)\n",
    "    joblib.dump(scaler, output_scaler_path)\n",
    "    logger.info(\"Model saved to %s\", output_model_path)\n",
    "    logger.info(\"Scaler saved to %s\", output_scaler_path)\n",
    "\n",
    "    return model, scaler, (X_test, y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bd46d77-213c-4d2c-8e96-45d60c400006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Evaluation utilities\n",
    "# ---------------------------\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    logger.info(\"Evaluation -> RMSE: %.4f | MAE: %.4f | R2: %.4f\", rmse, mae, r2)\n",
    "    return {'rmse': rmse, 'mae': mae, 'r2': r2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "578db2a1-ff7d-4c3c-8f7c-6dd3a3702b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Inference\n",
    "# ---------------------------\n",
    "def load_artifacts(model_path=MODEL_PATH, scaler_path=SCALER_PATH, embed_cache=EMBED_CACHE, empath_cols_path=SYMBOLS_CACHE):\n",
    "    \"\"\"Load trained model and scaler. Also load embedder from disk if available.\"\"\"\n",
    "    model = joblib.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    # load columns for empath space\n",
    "    if os.path.exists(empath_cols_path):\n",
    "        empath_cols = json.load(open(empath_cols_path, 'r'))\n",
    "    else:\n",
    "        empath_cols = None\n",
    "    embedder = None\n",
    "    try:\n",
    "        if os.path.exists(EMBEDDER_PATH):\n",
    "            embedder = joblib.load(EMBEDDER_PATH)\n",
    "    except Exception:\n",
    "        embedder = None\n",
    "    return model, scaler, embedder, empath_cols\n",
    "\n",
    "def infer_severity(text, model, scaler, embedder, empath_cols, empath_analyzer=None):\n",
    "    \"\"\"Single-text inference. Returns severity score (0-1).\"\"\"\n",
    "    # preprocess\n",
    "    txt = preprocess_keep_stopwords(text)\n",
    "    # embed\n",
    "    if embedder is None:\n",
    "        embedder = SentenceTransformer('all-MiniLM-L6-v2')  # fallback\n",
    "    emb = embedder.encode([txt])\n",
    "    emb = np.array(emb, dtype=np.float32)\n",
    "    # empath features\n",
    "    if empath_analyzer is None:\n",
    "        empath_analyzer = Empath()\n",
    "    empath_dict = empath_analyzer.analyze(txt, normalize=True)\n",
    "    # ensure columns order\n",
    "    if empath_cols is None:\n",
    "        empath_cols = sorted(list(empath_dict.keys()))\n",
    "    empath_row = [empath_dict.get(c, 0.0) for c in empath_cols]\n",
    "    empath_row = np.array(empath_row, dtype=np.float32).reshape(1, -1)\n",
    "    # combine\n",
    "    X_new = np.hstack([emb, empath_row])\n",
    "    # scale\n",
    "    X_new_scaled = scaler.transform(X_new)\n",
    "    # predict\n",
    "    pred = model.predict(X_new_scaled)[0]\n",
    "    return float(max(0.0, min(1.0, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1ae7ca9-50e6-437a-9484-b4a0c236a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Main execution\n",
    "# ---------------------------\n",
    "def main(input_csv=INPUT_CSV, overwrite_cache=False, test_size=0.2, device=None):\n",
    "    logger.info(\"Loading input CSV: %s\", input_csv)\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Expect columns\n",
    "    if 'processed_text' not in df.columns or 'severity_score' not in df.columns:\n",
    "        raise ValueError(\"Input CSV must contain 'processed_text' and 'severity_score' columns\")\n",
    "\n",
    "    # Preprocess (ensure stopwords are kept)\n",
    "    logger.info(\"Preprocessing texts (keep stopwords)...\")\n",
    "    df['processed_text_clean'] = df['processed_text'].fillna(\"\").astype(str).apply(preprocess_keep_stopwords)\n",
    "\n",
    "    texts = df['processed_text_clean'].tolist()\n",
    "    y = df['severity_score'].astype(float).values\n",
    "\n",
    "    # embeddings\n",
    "    if not overwrite_cache and os.path.exists(EMBED_CACHE):\n",
    "        embeddings = np.load(EMBED_CACHE)\n",
    "    else:\n",
    "        embeddings = compute_or_load_embeddings(texts, cache_path=EMBED_CACHE, device=device)\n",
    "\n",
    "    # empath features\n",
    "    if not overwrite_cache and os.path.exists(EMPATH_CACHE) and os.path.exists(SYMBOLS_CACHE):\n",
    "        empath_feats = np.load(EMPATH_CACHE, allow_pickle=True)\n",
    "        empath_cols = json.load(open(SYMBOLS_CACHE, 'r'))\n",
    "    else:\n",
    "        empath_analyzer = Empath()\n",
    "        empath_feats, empath_cols = build_empath_features(texts, empath_analyzer=empath_analyzer, cache_path=EMPATH_CACHE)\n",
    "\n",
    "    # Combine features (align dims)\n",
    "    logger.info(\"Combining embeddings (%s) + empath features (%s)...\", embeddings.shape, empath_feats.shape)\n",
    "    X = np.hstack([embeddings, empath_feats])\n",
    "\n",
    "    # Train model\n",
    "    model, scaler, test_triplet = train_model(X, y, output_model_path=MODEL_PATH, output_scaler_path=SCALER_PATH, test_size=test_size)\n",
    "    X_test, y_test, y_pred = test_triplet\n",
    "\n",
    "    # Evaluate\n",
    "    eval_stats = evaluate_preds(y_test, y_pred)\n",
    "\n",
    "    # Save additional artifacts (empath columns)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    json.dump({'empath_cols': empath_cols, 'embed_dim': embeddings.shape[1]}, open(os.path.join(OUTPUT_DIR, \"meta.json\"), 'w'))\n",
    "\n",
    "    logger.info(\"Pipeline finished. Model, scaler and artifacts saved to %s\", OUTPUT_DIR)\n",
    "    return model, scaler, embeddings, empath_feats, empath_cols, eval_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f4dbb10-eb04-4882-ab28-31507b8fd0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--input INPUT] [--outdir OUTDIR]\n",
      "                             [--overwrite] [--test_size TEST_SIZE]\n",
      "                             [--device DEVICE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\praja\\AppData\\Roaming\\jupyter\\runtime\\kernel-4b06ddd9-f94e-426f-abff-405b9badcebe.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praja\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# If run as script\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train severity model: embeddings + empath + LightGBM\")\n",
    "    parser.add_argument(\"--input\", type=str, default=INPUT_CSV, help=\"Input CSV path (must contain processed_text and severity_score)\")\n",
    "    parser.add_argument(\"--outdir\", type=str, default=OUTPUT_DIR, help=\"Output folder for models and caches\")\n",
    "    parser.add_argument(\"--overwrite\", action=\"store_true\", help=\"Overwrite caches (recompute embeddings/empath)\")\n",
    "    parser.add_argument(\"--test_size\", type=float, default=0.2, help=\"Test split fraction\")\n",
    "    parser.add_argument(\"--device\", type=str, default=None, help=\"Device for sentence-transformers (e.g. 'cuda' or 'cpu')\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # update global paths based on outdir\n",
    "    OUTPUT_DIR = args.outdir\n",
    "    EMBED_CACHE = os.path.join(OUTPUT_DIR, \"embeddings.npy\")\n",
    "    EMPATH_CACHE = os.path.join(OUTPUT_DIR, \"empath_feats.npy\")\n",
    "    SYMBOLS_CACHE = os.path.join(OUTPUT_DIR, \"empath_cols.json\")\n",
    "    MODEL_PATH = os.path.join(OUTPUT_DIR, \"lightgbm_severity.pkl\")\n",
    "    SCALER_PATH = os.path.join(OUTPUT_DIR, \"feature_scaler.pkl\")\n",
    "    EMBEDDER_PATH = os.path.join(OUTPUT_DIR, \"embedder_all-MiniLM.pkl\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    main(input_csv=args.input, overwrite_cache=args.overwrite, test_size=args.test_size, device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e72c2-0921-4365-99ac-dddae86b1332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
