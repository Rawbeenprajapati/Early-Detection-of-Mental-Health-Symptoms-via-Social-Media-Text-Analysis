{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d134f3-e23a-4868-88b5-d16999f60eef",
   "metadata": {},
   "source": [
    "# Echo ‚Äì listens and reflects feelings back, symbolizing understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446afb6c-1d53-44d6-81f6-fc28467cb3dd",
   "metadata": {},
   "source": [
    "**Chatbot Skeleton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30032b4-14f9-49b8-91cf-17473b0de581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import praw\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pre-processing libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import unicodedata\n",
    "import string\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from translatepy import Translator\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c4414c8-a7db-4b46-9450-be37c965f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(raw_text: str) -> str:\n",
    "    EMOTION_MAP = {\n",
    "        # üò¢ Sad / Negative / Depressed\n",
    "        \"üò≠\": \"crying\",\n",
    "        \"üíî\": \"broken_heart\",\n",
    "        \"üòî\": \"sad\",\n",
    "        \"üò¢\": \"tears\",\n",
    "        \"üòû\": \"disappointed\",\n",
    "        \"üòì\": \"anxious\",\n",
    "        \"üòü\": \"worried\",\n",
    "        \"üôÅ\": \"unhappy\",\n",
    "        \"üò©\": \"exhausted\",\n",
    "        \"üò´\": \"tired\",\n",
    "        \"üòñ\": \"frustrated\",\n",
    "        \"üò£\": \"distressed\",\n",
    "        \"ü•∫\": \"pleading\",\n",
    "        \"üòø\": \"crying_cat\",\n",
    "        \"‚òπÔ∏è\": \"frowning\",\n",
    "        \n",
    "        # üôÇ Neutral / Thinking\n",
    "        \"üòê\": \"neutral\",\n",
    "        \"ü§î\": \"thinking\",\n",
    "        \"üôÑ\": \"eye_roll\",\n",
    "        \n",
    "        # üòÄ Happy / Positive (less emphasized)\n",
    "        \"üòÇ\": \"laughing\",\n",
    "        \"üòÅ\": \"grinning\",\n",
    "        \"üòä\": \"happy\",\n",
    "        \"üòç\": \"love\",\n",
    "        \"‚ù§\": \"love\",\n",
    "        \"ü§ó\": \"hugging\",\n",
    "        \"üéâ\": \"celebration\"\n",
    "    }\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    translator = Translator()\n",
    "    \n",
    "    # ======================\n",
    "    # Helper Functions\n",
    "    # ======================\n",
    "    def expand_contractions(text):\n",
    "        return contractions.fix(text)\n",
    "    \n",
    "    def handle_special_characters(text):\n",
    "        text = str(text)\n",
    "        text = re.sub(r'[\\u200b-\\u200f\\u202a-\\u202e]', '', text)  # invisible chars\n",
    "        text = re.sub(r'[‚ù§‚ô°‚ô•]', 'heart', text)\n",
    "        text = re.sub(r'[‚òÖ‚òÜ‚úÆ‚úØ]', 'star', text)\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        return text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    def replace_emojis(text):\n",
    "        for emoji_char, label in EMOTION_MAP.items():\n",
    "            text = text.replace(emoji_char, f\" {label}\")\n",
    "        return text\n",
    "    \n",
    "    def translate_to_english(text):\n",
    "        try:\n",
    "            if not text.strip():\n",
    "                return text\n",
    "            result = translator.translate(text, \"English\")\n",
    "            translated_text = result.result\n",
    "            if translated_text.strip().lower() == text.strip().lower():\n",
    "                return text\n",
    "            else:\n",
    "                return translated_text\n",
    "        except Exception:\n",
    "            return text\n",
    "    text = str(raw_text).strip()\n",
    "    \n",
    "    # Replace emojis first\n",
    "    text = replace_emojis(text)\n",
    "    \n",
    "    # Translate, expand, handle special chars\n",
    "    text = text.lower()\n",
    "    text = translate_to_english(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = handle_special_characters(text)\n",
    "    \n",
    "    # Remove unwanted punctuation (keep underscores)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "\n",
    "    # trimming\n",
    "    text = ' '.join(text.split()[:400])\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = [t for t in text.split() if t not in stop_words]\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d98a519-5eb9-4e4f-8c68-cdd9bcdd407c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello laughing dark place last 2 years reached lowest point reflecting exactly point last 27 years life remaining potentially 40 look like quite happy person younger care world felt grounded reality enjoyed time friends family even though upbringing low socioeconomic even though raised single parent felt motivated felt great things would happen life failed high school squandered serious relationship ever wondered aimlessly 23 playing video games jobs fast forward money 5th year meant 4 year degree repeated 3rd year twice like 50k debt fail degree know man feel like even pass degree want job degree preparing know want know people normally illusion said someone 27 money barely work experience would eyebrows raising want functional human cannot find purpose cannot find anything drive reaching hopes someone maybe similar situation could would light go find purpose'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = preprocess_text(\"helloüòÇ dark place last 2 years reached lowest point reflecting exactly point last 27 years life remaining potentially 40 look like quite happy person younger care world felt grounded reality enjoyed time friends family even though upbringing low socioeconomic even though raised single parent felt motivated felt great things would happen life failed high school squandered serious relationship ever wondered aimlessly 23 playing video games jobs fast forward money 5th year meant 4 year degree repeated 3rd year twice like 50k debt fail degree know man feel like even pass degree want job degree preparing know want know people normally illusion said someone 27 money barely work experience would eyebrows raising want functional human cannot find purpose cannot find anything drive reaching hopes someone maybe similar situation could would light go find purpose\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd1766f-2920-4a62-87f3-e7a7fbc2b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_severity(text: str) -> float:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import numpy as np\n",
    "\n",
    "    # Load model globally so we don‚Äôt reload every call\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    labels = ['Negative', 'Neutral', 'Positive']\n",
    "    \n",
    "    # Valence and arousal mapping\n",
    "    valence_map = {'Positive': 1.0, 'Neutral': 0.0, 'Negative': -1.0}\n",
    "    arousal_map = {'Positive': 0.7, 'Neutral': 0.5, 'Negative': 0.8}\n",
    "    \n",
    "    def get_sentiment(text):\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "        output = model(**encoded_input)\n",
    "        scores = output.logits.detach().numpy()[0]\n",
    "        probs = np.exp(scores) / np.exp(scores).sum()\n",
    "        return dict(zip(labels, probs))\n",
    "    \n",
    "    sentiment = get_sentiment(text)\n",
    "\n",
    "    # Valence & arousal\n",
    "    valence = (\n",
    "        sentiment['Positive'] * valence_map['Positive'] +\n",
    "        sentiment['Neutral']  * valence_map['Neutral'] +\n",
    "        sentiment['Negative'] * valence_map['Negative']\n",
    "    )\n",
    "    arousal = (\n",
    "        sentiment['Positive'] * arousal_map['Positive'] +\n",
    "        sentiment['Neutral']  * arousal_map['Neutral'] +\n",
    "        sentiment['Negative'] * arousal_map['Negative']\n",
    "    )\n",
    "\n",
    "    # Raw severity\n",
    "    raw_severity = -valence * arousal\n",
    "    \n",
    "    # Normalize severity between 0 and 1 for easier interpretation\n",
    "    scaler = MinMaxScaler()\n",
    "    severity = scaler.fit_transform(np.array(raw_severity).reshape(-1, 1))\n",
    "\n",
    "    return float(severity[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ec26ff-7d86-437d-bb8b-1a0f97ec4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ==========================\n",
    "# Load Model Once\n",
    "# ==========================\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "# ==========================\n",
    "# Valence & Arousal Mapping\n",
    "# ==========================\n",
    "valence_map = {'Positive': 1.0, 'Neutral': 0.0, 'Negative': -1.0}\n",
    "arousal_map = {'Positive': 0.7, 'Neutral': 0.5, 'Negative': 0.8}\n",
    "\n",
    "# ==========================\n",
    "# Single Text Severity\n",
    "# ==========================\n",
    "def compute_severity(text: str) -> float:\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "\n",
    "    scores = output.logits.detach().numpy()[0]\n",
    "    probs = np.exp(scores) / np.exp(scores).sum()\n",
    "    sentiment = dict(zip(labels, probs))\n",
    "\n",
    "    # Weighted valence & arousal\n",
    "    valence = (\n",
    "        sentiment['Positive'] * valence_map['Positive'] +\n",
    "        sentiment['Neutral']  * valence_map['Neutral'] +\n",
    "        sentiment['Negative'] * valence_map['Negative']\n",
    "    )\n",
    "    arousal = (\n",
    "        sentiment['Positive'] * arousal_map['Positive'] +\n",
    "        sentiment['Neutral']  * arousal_map['Neutral'] +\n",
    "        sentiment['Negative'] * arousal_map['Negative']\n",
    "    )\n",
    "\n",
    "    # Raw severity\n",
    "    raw_severity = -valence * arousal\n",
    "\n",
    "    # Normalize raw severity to 0‚Äì1 using tanh (smooth scaling)\n",
    "    severity = (np.tanh(raw_severity) + 1) / 2\n",
    "\n",
    "    return float(severity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9025c54-cdaa-4533-8e32-d46e3b50871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8175044655799866\n"
     ]
    }
   ],
   "source": [
    "# text = \"life seriously worst like cannot say much people start judging say life like but seriously fucked every fucking negative trait life literally wreck trying fix years cannot energy continue everyone seems something going shit wtfwtf lowk want end icl cryingcryingbroken_heartbroken_heartbroken_heart\"\n",
    "text = \"I am not feeling well\"\n",
    "severity = compute_severity(text)\n",
    "\n",
    "print(severity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e29e4ab-b89a-43ea-9181-f59882336758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import numpy as np\n",
    "\n",
    "# class ConversationSeverityTracker:\n",
    "#     def __init__(self):\n",
    "#         self.raw_severities = []\n",
    "\n",
    "#     def add_message(self, text, compute_raw_severity):\n",
    "#         # Compute raw severity for this message\n",
    "#         raw = compute_raw_severity(text)  \n",
    "#         self.raw_severities.append(raw)\n",
    "\n",
    "#         # Rescale all severities so far\n",
    "#         scaler = MinMaxScaler()\n",
    "#         scaled = scaler.fit_transform(\n",
    "#             np.array(self.raw_severities).reshape(-1, 1)\n",
    "#         )\n",
    "\n",
    "#         # Return the scaled severity for the latest message\n",
    "#         return float(scaled[-1][0])\n",
    "\n",
    "# # Usage:\n",
    "# tracker = ConversationSeverityTracker()\n",
    "\n",
    "# msg1 = tracker.add_message(\"I‚Äôm gonna die lol\", compute_severity)\n",
    "# msg2 = tracker.add_message(\"nah just kidding\", compute_severity)\n",
    "# msg3 = tracker.add_message(\"but honestly I‚Äôm exhausted and can‚Äôt go on\", compute_severity)\n",
    "\n",
    "# print(msg1, msg2, msg3)  # progressively more accurate severities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42b124e6-4dc4-495b-9ad0-6b9039c7fccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Emotion detection\n",
    "emotion_model = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "\n",
    "# Topic detection\n",
    "topic_model = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "emotion = emotion_model(text)[0]['label']\n",
    "topic = topic_model(text, candidate_labels=[\"work\", \"relationship\", \"family\", \"health\", \"finance\"])['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac607da3-b162-4cd2-9661-5005030f9e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! I'm here to chat with you. Type 'exit' to end.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  how are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Detected topic: general\n",
      "Chatbot: It's okay to feel that way sometimes. Would you like to talk about it?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  I am having fight with my wife\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Detected topic: general\n",
      "Chatbot: It's okay to feel that way sometimes. Would you like to talk about it?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  I am parents don't like me\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Detected topic: family\n",
      "Chatbot: Sometimes family conflicts take an emotional toll√¢‚Ç¨‚Äùhow are you coping?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye! Take care.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Load topic-based responses\n",
    "with open(\"responses.json\", \"r\") as f:\n",
    "    topic_responses = json.load(f)\n",
    "\n",
    "# -----------------------------\n",
    "# Load or define severity thresholds\n",
    "# -----------------------------\n",
    "p75 = 0.86  # Moderate\n",
    "p90 = 0.94 # High\n",
    "p95 = 0.97 # Critical\n",
    "\n",
    "def assign_risk(sev):\n",
    "    if sev <= p75:\n",
    "        return \"Low\"\n",
    "    elif sev <= p90:\n",
    "        return \"Moderate\"\n",
    "    elif sev <= p95:\n",
    "        return \"High\"\n",
    "    else:\n",
    "        return \"Critical\"\n",
    "\n",
    "# -----------------------------\n",
    "# Placeholder functions\n",
    "# -----------------------------\n",
    "\n",
    "def alert_helpline(user_id, text, severity):\n",
    "    print(f\"ALERT: Critical risk detected for {user_id} | Severity: {severity:.2f}\")\n",
    "    # Here you can integrate with SMS/email/API to contact help line\n",
    "\n",
    "# -----------------------------\n",
    "# Logging function\n",
    "# -----------------------------\n",
    "def log_message(user_id, text, processed, severity, risk_level, file_path=\"output/chat_log_with_severity.csv\"):\n",
    "    df = pd.DataFrame([{\n",
    "        \"timestamp\": datetime.utcnow(),\n",
    "        \"user_id\": user_id,\n",
    "        \"text\": text,\n",
    "        \"Pre_processed_text\":processed,\n",
    "        \"severity\": severity,\n",
    "        \"risk_level\": risk_level\n",
    "    }])\n",
    "    \n",
    "    # If file exists, append without header. If not, write header.\n",
    "    if not os.path.isfile(file_path):\n",
    "        df.to_csv(file_path, mode='w', header=True, index=False)\n",
    "    else:\n",
    "        df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Background severity checker\n",
    "# -----------------------------\n",
    "def monitor_severity(user_id, text):\n",
    "    processed = preprocess_text(text)\n",
    "    severity = compute_severity(processed)\n",
    "    risk_level = assign_risk(severity)\n",
    "    \n",
    "    # Log the message\n",
    "    log_message(user_id, text, processed, severity, risk_level)\n",
    "    \n",
    "    # Trigger alert if critical\n",
    "    if risk_level == \"Critical\":\n",
    "        alert_helpline(user_id, processed, severity)\n",
    "    \n",
    "    return severity, risk_level\n",
    "\n",
    "# -----------------------------\n",
    "# Topic detection\n",
    "# -----------------------------\n",
    "def detect_topic(text):\n",
    "    text = text.lower()\n",
    "    topic_counter = Counter()\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        \n",
    "        # Count mentions for each topic\n",
    "        if any(word in words for word in [\"work\", \"job\", \"boss\", \"office\"]):\n",
    "            topic_counter[\"work\"] += 1\n",
    "        if any(word in words for word in [\"family\", \"parents\", \"siblings\"]):\n",
    "            topic_counter[\"family\"] += 1\n",
    "        if any(word in words for word in [\"school\", \"study\", \"exam\", \"university\", \"teacher\", \"assignment\", \"homework\"]):\n",
    "            topic_counter[\"study\"] += 1\n",
    "    \n",
    "    # Print counts of all detected topics\n",
    "    # print(\"Topic counts:\", dict(topic_counter))\n",
    "    \n",
    "    if topic_counter:\n",
    "        # Return the topic with highest count\n",
    "        main_topic = topic_counter.most_common(1)[0][0]\n",
    "    else:\n",
    "        main_topic = \"general\"\n",
    "    \n",
    "    return main_topic\n",
    "\n",
    "# -----------------------------\n",
    "# Chatbot main loop (simplified)\n",
    "# -----------------------------\n",
    "def chatbot():\n",
    "    user_id = \"user123\"\n",
    "    print(\"Chatbot: Hello! I'm here to chat with you. Type 'exit' to end.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Chatbot: Goodbye! Take care.\")\n",
    "            break\n",
    "        \n",
    "        # Start background thread for severity analysis\n",
    "        severity_thread = threading.Thread(target=monitor_severity, args=(user_id, user_input))\n",
    "        severity_thread.start()\n",
    "        \n",
    "        # Detect topic\n",
    "        topic = detect_topic(user_input)\n",
    "        # print(f\"[DEBUG] Detected topic: {topic}\")\n",
    "        \n",
    "        # Choose a random response based on topic\n",
    "        if topic in topic_responses:\n",
    "            response = random.choice(topic_responses[topic])\n",
    "        else:\n",
    "            response = random.choice(topic_responses[\"general\"])\n",
    "        \n",
    "        print(f\"Chatbot: {response}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Start chatbot\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439c13e-17d0-464e-b3fc-35f1279349f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
